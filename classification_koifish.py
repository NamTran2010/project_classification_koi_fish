# -*- coding: utf-8 -*-
"""classification_koiFish

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EEWa1Exv7f2-bz97lIjfDx2KeKfIEnjy
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import cv2 as cv
import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras import layers

!pip uninstall tensorflow

import pandas as pd # Xu lý bảng
import seaborn as sns # Vẽ biểu đồ thị của dữ liệu
import matplotlib.pyplot as plt 
from sklearn.preprocessing import StandardScaler # Xử lý chuẩn hóa dữ liệu
from sklearn.model_selection import train_test_split # Chia dữ liệu ra làm 2 phần
from keras.layers import Dense, Activation, Dropout, BatchNormalization, LSTM    # LSTM  biên dạng ANN, BatchNormalization: cho nhỏ lại
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical # Sử dung để làm nổi đối tượng cần phân loại
from keras import callbacks 
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score # Để đo lường

!sudo pip3 install keras

pip install tensorflow==2.7.0.

# Import Libraries
import tensorflow as tf
import matplotlib.pyplot as plt
import cv2 as cv
import os
import numpy as np
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.preprocessing import image
from tensorflow.keras import layers

import pandas as pd # Xu lý bảng
import seaborn as sns # Vẽ biểu đồ thị của dữ liệu
import matplotlib.pyplot as plt 
from sklearn.preprocessing import StandardScaler # Xử lý chuẩn hóa dữ liệu
from sklearn.model_selection import train_test_split # Chia dữ liệu ra làm 2 phần
from keras.layers import Dense, Activation, Dropout, BatchNormalization, LSTM    # LSTM  biên dạng ANN, BatchNormalization: cho nhỏ lại
from keras.models import Sequential
from tensorflow.keras.utils import to_categorical # Sử dung để làm nổi đối tượng cần phân loại
from keras import callbacks 
from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score # Để đo lường

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from tensorflow.keras.preprocessing import image
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping,ReduceLROnPlateau
import matplotlib.pyplot as plt
import tensorflow as tf
import cv2

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from keras import callbacks
import keras
from keras.layers import Dense # fully connected
from keras.datasets import boston_housing
from tensorflow.keras.optimizers import RMSprop # toi uu
from keras.callbacks import EarlyStopping # dung lai ngay lap tuc
from sklearn.preprocessing import scale # xu li du lieu

import matplotlib.pyplot as plt
from matplotlib.image import imread
from os import listdir
from numpy import asarray
from numpy import save
from keras.preprocessing.image import load_img, img_to_array
from keras.models import Sequential
from keras.layers import Dense, Activation, BatchNormalization, Dropout, Conv2D, MaxPooling2D, Flatten
import matplotlib.pyplot as plt
from tensorflow.keras.utils import load_img, img_to_array
import numpy as np

from google.colab import drive
drive.mount('/content/drive')

import sys
sys.path.append("drive")
import os
os.chdir('/content/drive/MyDrive/Colab Notebooks')

img = image.load_img("/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/train/Asagi/5100AsagiF33inches_533x_3_11zon.jpg")
plt.imshow(img)

import glob
Asagi = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Asagi/*.*')
Bekko = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Bekko/*.*')
Doitsu_Koi = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Doitsu koi/*.*')
Ghosiki = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Ghosiki/*.*')
Goromo = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Goromo/*.*')
Hikarimoyo = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Hikarimoyo/*.*')

Hikarimujimono = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Hikarimuji mono/*.*')
Hikariutsuri = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Hikariutsuri/*.*')
Kanoko_Koi = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Kanoko koi/*.*')
Kawarimono = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Kawarimono/*.*')
Kin_Ginrin = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Kin-Ginrin/*.*')
Kohaku = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Kohaku/*.*')

Sanke = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Sanke/*.*')
Showa = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Showa/*.*')
Shusui = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Shusui/*.*')
Tancho = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Tancho/*.*')
Utsuri = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Utsuri/*.*')
Yamato_Nishiki = glob.glob('/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Yamato Nishiki/*.*')

data = []
labels = []

for i in Asagi:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(0)
for i in Bekko:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(1)
for i in Doitsu_Koi:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(2)
for i in Ghosiki:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(3)
for i in Goromo:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(4)
for i in Hikarimoyo:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(5)
for i in Hikarimujimono:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(6)
for i in Hikariutsuri:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(7) 
for i in Kanoko_Koi:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(8)
for i in Kawarimono:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(9)
for i in Kin_Ginrin:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(10)
for i in Kohaku:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(11)
for i in Sanke:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(12)
for i in Showa:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(13)
for i in Shusui:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(14)
for i in Tancho:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(15)
for i in Utsuri:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(16)
for i in Yamato_Nishiki:   
    image=tf.keras.preprocessing.image.load_img(i, color_mode='rgb', 
    target_size= (150,150))
    image=np.array(image)
    data.append(image)
    labels.append(17)

data = np.array(data)
labels = np.array(labels)

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(data, labels, test_size=0.2,random_state=42)

print(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)

X_train = X_train.reshape((X_train.shape[0],150,150,3)).astype('float32')/255
X_test = X_test.reshape((X_test.shape[0],150,150,3)).astype('float32')/255

Y_train = to_categorical(Y_train,18)
Y_test = to_categorical(Y_test,18)

print(X_train)

print(Y_train)

from keras.layers import Conv2D, MaxPooling2D
model = Sequential()
model.add(Conv2D(32,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same',input_shape=(150,150,3)))
model.add(Conv2D(32,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(MaxPooling2D(2,2))



model.add(Conv2D(64,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 64 lan tich chap
model.add(Conv2D(64,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(MaxPooling2D(2,2))


model.add(Conv2D(128,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 128 lan tich chap
model.add(Conv2D(128,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(MaxPooling2D(2,2))



model.add(Conv2D(256,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same')) # 128 lan tich chap
model.add(Conv2D(256,(3,3), activation='relu',kernel_initializer='he_uniform',padding='same'))
model.add(MaxPooling2D(2,2))
model.add(Dropout(rate=0.5))


from keras.layers import Dense, Activation, Flatten
model.add(Flatten())
model.add(Dense(256, activation = 'relu', kernel_initializer='he_uniform'))
model.add(Dense(256, activation = 'relu', kernel_initializer='he_uniform'))
model.add(Dense(256, activation = 'relu', kernel_initializer='he_uniform'))
model.add(Dense(256, activation = 'relu', kernel_initializer='he_uniform'))
model.add(Dense(256, activation = 'relu', kernel_initializer='he_uniform'))
model.add(Dropout(rate=0.4))

model.add(Dense(18))
model.summary()

# Training
model.compile(loss='mse',optimizer='adam',metrics=['accuracy'])
history = model.fit(X_train, Y_train, epochs =120, batch_size =250,validation_data=(X_test,Y_test), verbose = 1)

# Save model
from tensorflow.keras.models import load_model
model.save('Final.h5')
model_ANN = load_model('Final.h5')

# Check accuracy
from tensorflow.keras.utils import load_img, img_to_array
import numpy as np
filename = "/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Utsuri/images (21)_1_11zon.jpg"

predict = ['Asagi','Bekko','Doitsu Koi','Ghosiki','Goromo','Hikarimoyo','Hikarimuji mono','Hikariutsuri','Kanoko Koi','Kawarimono','Kin/Ginrin','Kohaku','Sanke','Showa','Shusui','Tancho','Utsuri','Yamato Nishiki']
predict = np.array(predict)


img = load_img(filename,target_size=(150,150))
img = img_to_array(img)
img = img.reshape(1,150,150,3)
img = img.astype('float32')
img = img/255

result = np.argmax(model_ANN.predict(img),axis=-1)
predict[result]

# Check accuracy
from tensorflow.keras.utils import load_img, img_to_array
import numpy as np
filename = "/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Asagi/1.jpg.jpg"

predict = ['Asagi','Bekko','Doitsu Koi','Ghosiki','Goromo','Hikarimoyo','Hikarimuji mono','Hikariutsuri','Kanoko Koi','Kawarimono','Kin/Ginrin','Kohaku','Sanke','Showa','Shusui','Tancho','Utsuri','Yamato Nishiki']
predict = np.array(predict)


img = load_img(filename,target_size=(150,150))
img = img_to_array(img)
img = img.reshape(1,150,150,3)
img = img.astype('float32')
img = img/255

result = np.argmax(model_ANN.predict(img),axis=-1)
predict[result]

# Check accuracy
from tensorflow.keras.utils import load_img, img_to_array
import numpy as np
filename = "/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Bekko/bekko.jpg"

predict = ['Asagi','Bekko','Doitsu Koi','Ghosiki','Goromo','Hikarimoyo','Hikarimuji mono','Hikariutsuri','Kanoko Koi','Kawarimono','Kin/Ginrin','Kohaku','Sanke','Showa','Shusui','Tancho','Utsuri','Yamato Nishiki']
predict = np.array(predict)


img = load_img(filename,target_size=(150,150))
img = img_to_array(img)
img = img.reshape(1,150,150,3)
img = img.astype('float32')
img = img/255

result = np.argmax(model_ANN.predict(img),axis=-1)
predict[result]

# Check accuracy
from tensorflow.keras.utils import load_img, img_to_array
import numpy as np
filename = "/content/drive/MyDrive/Colab Notebooks/KOI_FISH_CLASSFICATION/data/KoiFish/test/Kanoko_koi/1_1_11zon.jpg"

predict = ['Asagi','Bekko','Doitsu Koi','Ghosiki','Goromo','Hikarimoyo','Hikarimuji mono','Hikariutsuri','Kanoko Koi','Kawarimono','Kin/Ginrin','Kohaku','Sanke','Showa','Shusui','Tancho','Utsuri','Yamato Nishiki']
predict = np.array(predict)


img = load_img(filename,target_size=(150,150))
img = img_to_array(img)
img = img.reshape(1,150,150,3)
img = img.astype('float32')
img = img/255

result = np.argmax(model_ANN.predict(img),axis=-1)
predict[result]

# Results
score = model.evaluate(X_test,Y_test, verbose=0)
print("Loss = ", score[0])
print("accuracy = ", score[1])

# Draw plot
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epochs')
plt.legend(['train','Validation'])
plt.show()

# Draw plot
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model accuracy')
plt.ylabel('loss')
plt.xlabel('epochs')
plt.legend(['train','Validation'])
plt.show()